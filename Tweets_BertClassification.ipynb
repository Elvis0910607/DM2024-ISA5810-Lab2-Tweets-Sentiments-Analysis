{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"dockerImageVersionId":30806,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom transformers import (BertTokenizerFast,TFBertTokenizer,BertTokenizer,RobertaTokenizerFast,\n                          DataCollatorWithPadding,TFRobertaForSequenceClassification,TFBertForSequenceClassification,\n                          TFBertModel,create_optimizer)\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T08:06:31.412009Z","iopub.execute_input":"2024-12-14T08:06:31.412396Z","iopub.status.idle":"2024-12-14T08:06:50.237223Z","shell.execute_reply.started":"2024-12-14T08:06:31.412344Z","shell.execute_reply":"2024-12-14T08:06:50.236493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\n \nf.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T08:06:50.238850Z","iopub.execute_input":"2024-12-14T08:06:50.239771Z","iopub.status.idle":"2024-12-14T08:07:18.053406Z","shell.execute_reply.started":"2024-12-14T08:06:50.239719Z","shell.execute_reply":"2024-12-14T08:07:18.052670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_data = [\n    {\n        \"_score\": item[\"_score\"],\n        \"_index\": item[\"_index\"],\n        \"_crawldate\": item[\"_crawldate\"],\n        \"_type\": item[\"_type\"],\n        \"hashtags\": item[\"_source\"][\"tweet\"].get(\"hashtags\", []),\n        \"tweet_id\": item[\"_source\"][\"tweet\"].get(\"tweet_id\"),\n        \"text\": item[\"_source\"][\"tweet\"].get(\"text\"),\n    }\n    for item in data\n]\ntweets = pd.DataFrame(processed_data)\n\nprint(tweets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T08:07:18.054470Z","iopub.execute_input":"2024-12-14T08:07:18.054813Z","iopub.status.idle":"2024-12-14T08:07:26.967585Z","shell.execute_reply.started":"2024-12-14T08:07:18.054778Z","shell.execute_reply":"2024-12-14T08:07:26.966657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_path = '/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv'\ndata_identification_path = '/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv'\nemotion_df = pd.read_csv(emotion_path)\ndata_identification_df = pd.read_csv(data_identification_path)\n\nprint(emotion_df.shape)\nprint(data_identification_df.shape)\nprint(tweets.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T08:07:26.969569Z","iopub.execute_input":"2024-12-14T08:07:26.969867Z","iopub.status.idle":"2024-12-14T08:07:28.826690Z","shell.execute_reply.started":"2024-12-14T08:07:26.969838Z","shell.execute_reply":"2024-12-14T08:07:28.825574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#merge training data\ntrain_df = data_identification_df[data_identification_df['identification'] == 'train']\nmerged_df = pd.merge(train_df, emotion_df, on='tweet_id', how='inner')\ntrain_df = pd.merge(merged_df, tweets, on='tweet_id', how='inner')\ncolumns_to_keep = ['tweet_id', 'emotion', '_score', 'hashtags', 'text']\ntrain_df = train_df[columns_to_keep]\nprint(train_df)\ntext = train_df['text']\nemotion = train_df['emotion']\n\n# merge test data\ntest_df = data_identification_df[data_identification_df['identification'] == 'test']\ntest_df = pd.merge(test_df, tweets, on='tweet_id', how='inner')\ncolumns_to_keep = ['tweet_id', '_score', 'hashtags', 'text']\ntest_df = test_df[columns_to_keep]\nprint(test_df)\n\nprint(train_df)\nprint(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T08:07:28.827727Z","iopub.execute_input":"2024-12-14T08:07:28.827956Z","iopub.status.idle":"2024-12-14T08:07:39.074492Z","shell.execute_reply.started":"2024-12-14T08:07:28.827932Z","shell.execute_reply":"2024-12-14T08:07:39.073460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = train_df.groupby('emotion', group_keys=False).apply(\n    lambda x: x.sample(frac=700000 / len(train_df))\n).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:22:12.902270Z","iopub.execute_input":"2024-12-07T08:22:12.902565Z","iopub.status.idle":"2024-12-07T08:22:13.461915Z","shell.execute_reply.started":"2024-12-07T08:22:12.902537Z","shell.execute_reply":"2024-12-07T08:22:13.460784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train_data = train_df ['emotion']\nX_train_data = train_df ['text']\n\nx_train, x_val, y_train, y_val =  train_test_split(X_train_data[:], \n                      y_train_data[:], test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:22:13.463307Z","iopub.execute_input":"2024-12-07T08:22:13.463908Z","iopub.status.idle":"2024-12-07T08:22:13.480056Z","shell.execute_reply.started":"2024-12-07T08:22:13.463849Z","shell.execute_reply":"2024-12-07T08:22:13.479210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer\nimport re\nimport emoji\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\n# Define cleaning function\ndef clean_tweet(text, emoji_dict):\n    # Replace defined emojis with corresponding keywords\n    for emj, keyword in emoji_dict.items():\n        text = text.replace(emj, keyword)\n    # Remove remaining emojis\n    text = emoji.replace_emoji(text, replace='')\n    # Remove <LH> tags\n    text = re.sub(r'<LH>', '', text)\n    # Remove characters starting with @ (e.g., @username)\n    text = re.sub(r'@\\w+', '', text)\n    # Remove punctuation marks\n    text = re.sub(r'[^\\w\\s]', '', text) \n    # Remove extra whitespace characters\n    text = text.strip()\n    return text\n\n# Define a dictionary for emoji mappings\nemoji_dict = {\n    'üòÇ': '[joy]',\n    '‚ù§Ô∏è': '[love]',\n    'üòç': '[adoration]',\n    'üò≠': '[cry]',\n    '‚ù§': '[care]',\n    'üòä': '[happy]',\n    'üôè': '[pray]',\n    'üòò': '[kiss]',\n    'üíï': '[love_each_other]',\n    'üî•': '[fire]',\n    'üò©': '[weary]',\n    'ü§î': '[think]',\n    'üíØ': '[perfect]',\n    'üíô': '[loyalty]',\n    'üôÑ': '[annoyed]',\n    'üòÅ': '[happy]',\n    'üôå': '[celebrate]',\n    'üôèüèæ': '[pray]',\n    'üëç': '[approve]',\n    'üôèüèΩ': '[pray]'\n}\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\n# First, clean the training and test sets\ntest_df['tokenized_text'] = [clean_tweet(text, emoji_dict) for text in test_df['text']]\nx_train = [clean_tweet(text, emoji_dict) for text in x_train]\nx_val = [clean_tweet(text, emoji_dict) for text in x_val]\n# Tokenize the cleaned text\nx_train_encoding = tokenizer(x_train, truncation=True, padding=True, max_length=64)\nx_val_encoding = tokenizer(x_val, truncation=True, padding=True, max_length=64)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:22:13.481291Z","iopub.execute_input":"2024-12-07T08:22:13.481741Z","iopub.status.idle":"2024-12-07T08:22:53.264536Z","shell.execute_reply.started":"2024-12-07T08:22:13.481711Z","shell.execute_reply":"2024-12-07T08:22:53.263820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_val_encoded = label_encoder.fit_transform(y_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:22:53.265651Z","iopub.execute_input":"2024-12-07T08:22:53.266022Z","iopub.status.idle":"2024-12-07T08:22:53.273401Z","shell.execute_reply.started":"2024-12-07T08:22:53.265982Z","shell.execute_reply":"2024-12-07T08:22:53.272447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\nclass NewsDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    # Read a single sample\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(int(self.labels[idx]))\n        return item\n    \n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = NewsDataset(x_train_encoding, y_train_encoded)\ntest_dataset = NewsDataset(x_val_encoding, y_val_encoded)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:22:53.274450Z","iopub.execute_input":"2024-12-07T08:22:53.274704Z","iopub.status.idle":"2024-12-07T08:22:53.290046Z","shell.execute_reply.started":"2024-12-07T08:22:53.274680Z","shell.execute_reply":"2024-12-07T08:22:53.289326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nmodel = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=8)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n \ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n \noptim = AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_loader) * 1\nscheduler = get_linear_schedule_with_warmup(optim, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:22:53.290975Z","iopub.execute_input":"2024-12-07T08:22:53.291218Z","iopub.status.idle":"2024-12-07T08:22:54.655707Z","shell.execute_reply.started":"2024-12-07T08:22:53.291193Z","shell.execute_reply":"2024-12-07T08:22:54.654921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    # Get the index of the label with the highest probability in the prediction results\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    # Flatten the true labels to one dimension\n    labels_flat = labels.flatten()\n    # Compare the predicted and true labels, calculate the number of correct predictions\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:22:54.657499Z","iopub.execute_input":"2024-12-07T08:22:54.657752Z","iopub.status.idle":"2024-12-07T08:22:54.662374Z","shell.execute_reply.started":"2024-12-07T08:22:54.657723Z","shell.execute_reply":"2024-12-07T08:22:54.661425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm  # Used to display progress bars\n\n# Training function\ndef train():\n    model.train()\n    total_train_loss = 0\n    iter_num = 0\n    total_iter = len(train_loader)\n    \n    # Display progress bar using tqdm\n    for batch in tqdm(train_loader, desc=\"Training\", total=total_iter):\n        # Forward pass\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        total_train_loss += loss.item()\n        \n        # Backpropagation\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Parameter update\n        optim.step()\n        scheduler.step()\n \n        iter_num += 1\n        if iter_num % 100 == 0:\n            print(\"Epoch: %d, Iteration: %d, Loss: %.4f, %.2f%%\" % (epoch,\n                            iter_num, loss.item(), iter_num/total_iter*100))\n        \n    avg_train_loss = total_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)  # Save training loss\n    print(\"Epoch: %d, Average training loss: %.4f\" % (epoch, avg_train_loss))\n\n# Validation function\ndef validation():\n    model.eval()\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    for batch in tqdm(test_dataloader, desc=\"Validating\", total=len(test_dataloader)):\n        with torch.no_grad():\n            # Forward pass\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        loss = outputs[0]\n        logits = outputs[1]\n        \n        # Calculate total loss\n        total_eval_loss += loss.item()\n        logits = logits.detach().cpu().numpy()  # Move logits to CPU and convert to numpy array\n        label_ids = labels.to('cpu').numpy()  # Move true labels to CPU and convert to numpy array\n        # Calculate accuracy using flat_accuracy function\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n    \n    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n    avg_val_loss = total_eval_loss / len(test_dataloader)\n    \n    # Save validation loss\n    val_losses.append(avg_val_loss)\n    \n    print(\"Validation Accuracy: %.4f\" % (avg_val_accuracy))\n    print(\"Average Validation Loss: %.4f\" % (avg_val_loss))\n    print(\"-------------------------------\")\n\n# Lists to store training and validation losses\ntrain_losses = []\nval_losses = []\n\n# Execute training and validation\nfor epoch in range(4):\n    print(\"------------Epoch: %d ----------------\" % epoch)\n    train()\n    validation()\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Validation Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Losses')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:22:54.663725Z","iopub.execute_input":"2024-12-07T08:22:54.664005Z","iopub.status.idle":"2024-12-07T08:23:53.824825Z","shell.execute_reply.started":"2024-12-07T08:22:54.663979Z","shell.execute_reply":"2024-12-07T08:23:53.823932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm  # Import tqdm progress bar library\n\n# Model prediction function\ndef predict():\n    \n    model.eval()\n    predictions = []\n    \n    # Disable gradient calculations\n    with torch.no_grad():\n        # Initialize tqdm progress bar, setting the total steps to the total number of samples \n        #in the dataset\n        with tqdm(total=len(test_df), desc=\"Predicting\", ncols=100) as pbar:\n            for batch in test_dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                \n                # Forward pass\n                outputs = model(input_ids, attention_mask=attention_mask)\n                logits = outputs[0]\n                \n                # Get the predicted labels (based on the maximum value of the softmax output)\n                predicted_labels = torch.argmax(logits, dim=-1).cpu().numpy()\n                \n                # Update predictions and progress bar\n                predictions.extend(predicted_labels)\n                pbar.update(len(predicted_labels))  # Update progress bar with the number \n                                                    # of predicted samples\n    \n    return predictions\n\n# Data preprocessing and DataLoader setup\ntest_encodings = tokenizer(\n    test_df['tokenized_text'].tolist(), \n    truncation=True, \n    padding=True, \n    max_length=64\n)\ntest_dataset = NewsDataset(test_encodings, [0] * len(test_df))  # Create test dataset\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Call the prediction function\npredicted_labels = predict()\n\n# Convert the predicted numerical labels back to the original labels\npredicted_labels = label_encoder.inverse_transform(predicted_labels)\n\n# Add the prediction results to the DataFrame\ntest_df['predicted_labels'] = predicted_labels\n\n# Save the prediction results to a CSV file\nsubmission = test_df[['tweet_id', 'predicted_labels']].rename(columns={\n    'tweet_id': 'id',\n    'predicted_labels': 'emotion'\n})\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:23:53.828642Z","iopub.execute_input":"2024-12-07T08:23:53.828948Z","iopub.status.idle":"2024-12-07T08:39:04.231215Z","shell.execute_reply.started":"2024-12-07T08:23:53.828920Z","shell.execute_reply":"2024-12-07T08:39:04.230107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T08:39:04.232520Z","iopub.execute_input":"2024-12-07T08:39:04.232813Z","iopub.status.idle":"2024-12-07T08:39:04.242376Z","shell.execute_reply.started":"2024-12-07T08:39:04.232786Z","shell.execute_reply":"2024-12-07T08:39:04.241580Z"}},"outputs":[],"execution_count":null}]}